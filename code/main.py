import  pandas as pd
import random
import math
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.decomposition import KernelPCA
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import multilabel_confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
from typing import List, Tuple, Literal, cast
from matplotlib.axes import Axes
from matplotlib.lines import Line2D

if __name__ == "__main__":

    #loading training dataset
    dfadult = pd.read_csv("data/adult.data", 
                        names=["age","workclass","fnlwgt",
                                "education","education-num","marital-status",
                                "occupation","relationship","race",
                                "sex","capital-gain","capital-loss",
                                "hours-per-week","native-country","class"],
                        header=None)
    dfadult.info()


    #checking for null columns in target and discarding its rows -- none have been found
    print("na % in class : ", dfadult["class"].isna().sum() / dfadult.shape[0])
    dfadult.dropna(subset=["class"])

    #splitting target from features
    y = dfadult["class"].str.strip()
    X = dfadult.drop(columns=["class"])
    for col in X.select_dtypes(include="object").columns:
        X[col] = X[col].str.strip()

    #checking for null columns in features -- none have been found
    [print("na % in", col, ": ", dfadult[col].isna().sum() / dfadult.shape[0]) 
     for col in X.columns.values]
    
    #separating num columns from object ones
    num_col = [i for i, col in enumerate(X.columns) if pd.api.types.is_numeric_dtype(X[col])]
    ohe_col = [i for i, col in enumerate(X.columns) if not pd.api.types.is_numeric_dtype(X[col])]
    #X.drop(columns=X.columns[ohe_col])
    
    #generating a set of 10 random seeds to split the dataset
    if False: [print(i,":",random.randint(0,1_000_000)) for i in range(1,11)] #Set to True to generate seeds

    #list of seeds generated by the code above
    SEEDS = [336945,845681,547312,930030,894227,902725,780836,498052,862845,72918]
    split_seeds_idx = 3


    #declaring the list of models
    models = []

    pca_comps = {
        None: [None],
        "linear": [3, 6, 9],
        "poly": [4, 8, 12],
        "rbf": [5, 10, 15, 20]
    }
    for ker in [None]:
        for comp in pca_comps[ker]:
            for cr in [.1, 1, 10]:
                estimators = []
                num_pipe:List[Tuple[str, object]] = [("scl", StandardScaler())]
                if not ker is None: num_pipe.append(("kpca", KernelPCA(kernel=ker, n_components=comp)))
                estimators.append(("preprocess", ColumnTransformer(transformers=
                                    [("num", Pipeline(num_pipe), num_col), 
                                        ("ohe", OneHotEncoder(handle_unknown="ignore"), ohe_col)
                                        ])))                                  
                estimators.append(("mod",LogisticRegression(C=cr, l1_ratio=.0, solver="lbfgs",max_iter=200)))
                models.append(("logreg", ker, comp, cr, None, Pipeline(estimators)))

    for ker in [None]:
        for comp in pca_comps[ker]:
            for ne in [200, 400, 600]:
                for crt in ["gini", "entropy", "log_loss"]:
                    estimators = []
                    num_pipe:List[Tuple[str, object]] = [("passthrough", "passthrough")]
                    if not ker is None: num_pipe.append(("kpca",KernelPCA(kernel=ker, n_components=comp)))
                    estimators.append(("preprocess", ColumnTransformer(transformers=
                                        [("num", Pipeline(num_pipe), num_col), 
                                         ("ohe", OneHotEncoder(handle_unknown="ignore"), ohe_col)])))
                    estimators.append(("mod",RandomForestClassifier(n_estimators=ne, 
                                                                    criterion=cast(Literal["gini", "entropy", "log_loss"], crt))))
                    models.append(("rdforest", ker, comp, ne, crt, Pipeline(estimators)))

    for ker in [None]:
        for comp in pca_comps[ker]:
            for ne in [200, 400, 600]:
                for ss in [.4, .7, 1]:
                    estimators = []
                    num_pipe:List[Tuple[str, object]] = [("passthrough", "passthrough")]
                    if not ker is None: num_pipe.append(("kpca",KernelPCA(kernel=ker, n_components=comp)))
                    estimators.append(("preprocess", ColumnTransformer(transformers=
                                        [("num", Pipeline(num_pipe), num_col), 
                                         ("ohe", OneHotEncoder(handle_unknown="ignore"), ohe_col)])))
                    estimators.append(("mod",GradientBoostingClassifier(n_estimators=ne, subsample=ss)))
                    models.append(("gboost", ker, comp, ne, ss, Pipeline(estimators)))

    for hls in [(32,),(64,),(128,),(64,32)]:
        for al in [1e-4, 1e-3]:
            estimators = []
            estimators.append(("preprocess", ColumnTransformer(transformers=
                                    [("scl",StandardScaler(), num_col), 
                                        ("ohe", OneHotEncoder(handle_unknown="ignore"), ohe_col)])))
            estimators.append(("mod",MLPClassifier(hidden_layer_sizes=hls, 
                                                    activation=cast(Literal["logistic", "tanh","relu"], "relu"), alpha=al)))
            models.append(("mlp", None, None, hls, al, Pipeline(estimators)))

    
    #first training step to filter best models
    results = []
    #for each seed
    for cnt, seedval in enumerate(SEEDS[:split_seeds_idx]):
        print("Seed ",cnt + 1,"/",len(SEEDS[:split_seeds_idx]))
        seed_results = []

        #splitting X and y in train and validation
        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seedval)

        model:Pipeline
        #for each model
        for cnt_m, (mname, ker, _, _, _, model) in enumerate(models):
            print("Model ",cnt_m + 1,"/",len(models))
            #training model on X_train
            model.fit(X_train, y_train)

            #computing metrics on X_val
            y_val_pred = model.predict(X_val)
            
    
            seed_results.append(multilabel_confusion_matrix(y_true=y_val, 
                                                                y_pred=y_val_pred, 
                                                                labels=["<=50K",">50K"]))
            
        results.append(seed_results)

    #mean of results
    cms = np.array(results) #(S,M,C,2,2)
    cms = cms.sum(axis=2) #(S,M,2,2)
    flat_cms = cms.reshape(cms.shape[0],cms.shape[1], 4) #(S,M,4)

    means = flat_cms.mean(axis=0) #(M,4)
    acc = (means[:,0] + means[:,3]) / means.sum(axis=1) #(M)


    #declaring the list of filtered models
    step2_models = []
    families = sorted({model[0] for model in models})
    #foreach mname in list
    for family in families:
        #adding to step2_models ceiling top 30% with mask mname
        fam_indices = [i for i, model in enumerate(models) if model[0] == family]
        fam_acc = acc[fam_indices]
        k = math.ceil(.3 * len(fam_indices))
        top_k = [models[fam_indices[i]] for i in np.argsort(fam_acc)[-k:]]
        step2_models.extend(top_k)

    #second training step to filter most reliable models
    results = []
    #for each seed
    for cnt, seedval in enumerate(SEEDS[split_seeds_idx:]):
        print("S2 Seed ",cnt + 1,"/",len(SEEDS[split_seeds_idx:]))
        seed_results = []

        #splitting X and y in train and validation
        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seedval)

        model:Pipeline
        #for each model
        for cnt_m, (mname, ker, _, _, _, model) in enumerate(step2_models):
            print("Model ",cnt_m + 1,"/",len(step2_models))
            #training model on X_train
            model.fit(X_train, y_train)

            #computing metrics on X_val
            y_val_pred = model.predict(X_val)
    
            seed_results.append(multilabel_confusion_matrix(y_true=y_val, 
                                                               y_pred=y_val_pred, 
                                                               labels=["<=50K",">50K"]))

        #add the mode as baseline
        values, counts = np.unique(y_train, return_counts=True)
        mode = values[counts.argmax()]
        seed_results.append(multilabel_confusion_matrix(y_true=y_val, 
                                                               y_pred=np.full_like(y_val,fill_value=mode), 
                                                               labels=["<=50K",">50K"]))
        results.append(seed_results)

    #add baseline to models
    step2_models.append(("mode", None, None, None, None, None))

    #mean and variance of results
    cms = np.array(results) #(S,M,2,2,2)
    cms = cms.sum(axis=2) #(S,M,2,2)
    flat_cms = cms.reshape(cms.shape[0],cms.shape[1], 4) #(S,M,4)
    totals = flat_cms.sum(axis=2, keepdims=True)
    flat_cms = flat_cms / totals
    means = flat_cms.mean(axis=0) #(M,4)
    stds = flat_cms.std(axis=0) #(M,4)

    #filtering top 2 of each family promoting stability
    acc = cast(np.ndarray, (flat_cms[:,:,0] + flat_cms[:,:,3]) / flat_cms.sum(axis=2)) #(S,M)
    acc_means = acc.mean(axis=0) #(M)
    acc_std = acc.std(axis=0) #(M)
    lambdaa = 1.5 # scales the number of std considered
    k2 = 2 # top 2 models per family at most
    scores = acc_means - lambdaa * acc_std

    best_models = []
    families = sorted({model[0] for model in step2_models})
    #foreach mname in list
    for family in families:
        #add to best_models top2 in cms with mask mname
        fam_indices = [i for i, model in enumerate(step2_models) if model[0] == family]
        fam_scores = scores[fam_indices]
        k = min(k2, len(fam_scores))
        top_k = [(*step2_models[fam_indices[i]][:5], means[fam_indices[i]], stds[fam_indices[i]])
                  for i in np.argsort(fam_scores)[-k:]]        
        best_models.extend(top_k)

    
    #plot per model stats
    cm_labels = ["TP","FP","FN","TN"]
    x_base = np.arange(len(best_models))
    
    family_colors = {
        "logreg":"tab:blue",
        "rdforest":"tab:orange",
        "gboost":"tab:green",
        "mlp":"tab:red",
        "mode":"black"
    }

    family_offsets = {
        "logreg": -0.15,
        "rdforest": -0.05,
        "gboost": 0.05,
        "mlp": 0.15,
        "mode": 0.0
    }

    axes: List[Axes]
    fig, axes = plt.subplots(1,4,figsize=(20,5),sharey=True)

    for cm_idx, ax in enumerate(axes):
        for i, model in enumerate(best_models):
            family, _, _, _, _, mean_cm, std_cm = model

            x = x_base[i] + family_offsets[family]
            
            plt_std_cm = 3 * std_cm[cm_idx]
            ax.errorbar(
                x,
                mean_cm[cm_idx],
                yerr=plt_std_cm,
                fmt="none",
                color=family_colors[family],
                capsize=6,
                alpha=.9,
                zorder=1
            )

            ax.scatter(
                x,
                mean_cm[cm_idx],
                color=family_colors[family],
                s=40,
                zorder=2
            )

        ax.set_title(cm_labels[cm_idx])
        ax.set_xticks(x_base)
        ax.set_xticklabels(
            [
                "_".join([model[0], 
                         str(model[1]), 
                         str(model[2]), 
                         str(model[3]), 
                         str(model[4])])
                for model in best_models
            ],
            rotation=45,
            ha="right",
            fontsize=9
        )
        ax.grid(axis="y", linestyle="--", alpha=.4)

    legend_handles = [
        Line2D(
            [0], [0], marker="o",linestyle="",
            color=family_colors[family], label=family
        )
        for family in family_colors
    ]

    fig.legend(
        handles=legend_handles,
        loc="upper center",
        ncol=len(legend_handles),
        frameon=False,
        bbox_to_anchor=(0.5, 1.02)
    )

    fig.suptitle("Confusion Matrix of best Models (mean +- 3std)", y=1.05)
    fig.savefig("fig/plot.png", bbox_inches="tight")
    plt.tight_layout()
    plt.show()

    print("OK")